{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876954ed",
   "metadata": {},
   "source": [
    "# Fine-Tuning Language Models from Human Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc87d6f",
   "metadata": {},
   "source": [
    "Обзор работы по по использованию асесорской ифнормации для дообучения модели на основе обучения с подкреплением. [Статья по ссылке](https://arxiv.org/pdf/1909.08593.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4406f9",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebc0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3980cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убирем рандом\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da4c740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df962e51",
   "metadata": {},
   "source": [
    "# Базовый пример обучения генерации парафраза\n",
    "\n",
    "В работе рассматривается задача сумаризации, а это немного сложно... рассмотри их подход для задачи генерации парафраза... это немного проще..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d3939",
   "metadata": {},
   "source": [
    "### Скачиваем данные для парафраза на английском.\n",
    "[Статья 2021 года. ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase Generation.](https://github.com/dqxiu/ParaSCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370e898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/dqxiu/ParaSCI/master/Data/ParaSCI-ACL/'\n",
    "train_urls = ('train/train.src', 'train/train.tgt')\n",
    "val_urls = ('val/val.src', 'val/val.tgt')\n",
    "test_urls = ('test/test.src', 'test/test.tgt')\n",
    "\n",
    "train_filepaths = [download_from_url(url_base + url) for url in train_urls]\n",
    "val_filepaths = [download_from_url(url_base + url) for url in val_urls]\n",
    "test_filepaths = [download_from_url(url_base + url) for url in test_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c018b5b",
   "metadata": {},
   "source": [
    "### Строим словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d00d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def build_vocab(filepaths, tokenizer):\n",
    "    counter = Counter()\n",
    "    for filepath in filepaths:\n",
    "        with io.open(filepath, encoding=\"utf8\") as f:\n",
    "            for string_ in f:\n",
    "                counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "vocab = build_vocab(train_filepaths, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c17ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14821"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d30be2",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2dd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_tgt_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for (raw_src, raw_tgt) in zip(raw_src_iter, raw_tgt_iter):\n",
    "        src_tensor_ = torch.tensor(\n",
    "            [vocab[token] for token in tokenizer(raw_src.rstrip(\"\\n\"))],\n",
    "            dtype=torch.long)\n",
    "        tgt_tensor_ = torch.tensor(\n",
    "            [vocab[token] for token in tokenizer(raw_tgt.rstrip(\"\\n\"))],\n",
    "            dtype=torch.long)\n",
    "        data.append((src_tensor_, tgt_tensor_))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09e4a3",
   "metadata": {},
   "source": [
    "### Финальные датасеты для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48b8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "PAD_IDX = vocab['<pad>']\n",
    "BOS_IDX = vocab['<bos>']\n",
    "EOS_IDX = vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f148047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for (src_item, tgt_item) in data_batch:\n",
    "        src_batch.append(torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        tgt_batch.append(torch.cat([torch.tensor([BOS_IDX]), tgt_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6799c3",
   "metadata": {},
   "source": [
    "### Класс модели SEQ2SEQ transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff66cc",
   "metadata": {},
   "source": [
    "#### Определение самой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03183b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
    "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
    "                                        tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding +\n",
    "                            self.pos_embedding[:token_embedding.size(0),:])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "# Делаем, так чтобы в обучении не было заглядывания на дальнешие слова\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d1258",
   "metadata": {},
   "source": [
    "#### Определения декодирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0a8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, num_samples=1):\n",
    "    src = src.to(DEVICE)\n",
    "    src = torch.cat([src]*num_samples, dim=1)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, num_samples).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(DEVICE).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.detach()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        next_word.view(1, -1)], dim=0)\n",
    "    return ys.transpose(0,1)\n",
    "\n",
    "def sampling_decode(model, src, src_mask, max_len, start_symbol, num_samples=1):\n",
    "    src = src.to(DEVICE)\n",
    "    src = torch.cat([src]*num_samples, dim=1)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, num_samples).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(DEVICE).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = torch.multinomial(torch.nn.functional.softmax(prob, dim=-1), 1)\n",
    "        next_word = next_word.detach()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        next_word.view(1, -1)], dim=0)\n",
    "    return ys.transpose(0,1)\n",
    "\n",
    "def paraphrase(model, \n",
    "              srcs, \n",
    "              src_vocab, \n",
    "              tgt_vocab, \n",
    "              src_tokenizer, \n",
    "              decoder=greedy_decode, \n",
    "              ret_tokens=False, \n",
    "              ret_idx=False, \n",
    "              max_len_add=10,\n",
    "              input_idx=False,\n",
    "              **argv):\n",
    "    model.eval()\n",
    "    global_answers = []\n",
    "    for src in srcs:\n",
    "        if not input_idx:\n",
    "            tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
    "            src = torch.LongTensor(tokens)\n",
    "        num_tokens = len(src)\n",
    "        src = src.reshape(num_tokens, 1)\n",
    "        \n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "        tgt_tokens = decoder(model, src, src_mask, max_len=num_tokens + max_len_add, start_symbol=BOS_IDX, **argv)\n",
    "\n",
    "        answers = []\n",
    "        for tgt_token in tgt_tokens:\n",
    "            if not ret_idx:\n",
    "                reference = []\n",
    "                for tok in tgt_token:\n",
    "                    if tok.item() == tgt_vocab['<eos>']:\n",
    "                        break\n",
    "                    if tok.item() not in {tgt_vocab['<eos>'], tgt_vocab['<bos>'], tgt_vocab['<pad>']}:\n",
    "                        reference.append(tgt_vocab.itos[tok])\n",
    "                answers.append(\" \".join(reference).strip())\n",
    "                if ret_tokens:\n",
    "                    answers[-1] = answers[-1].split(\" \")\n",
    "            else:\n",
    "                reference = []\n",
    "                for tok in tgt_token:\n",
    "                    if tok.item() == tgt_vocab['<eos>']:\n",
    "                        break\n",
    "                    if tok.item() not in {tgt_vocab['<eos>'], tgt_vocab['<bos>'], tgt_vocab['<pad>']}:\n",
    "                        reference.append(tok.item())\n",
    "                        \n",
    "                answers.append(reference)\n",
    "        global_answers.append(answers)\n",
    "    return global_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d37fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses / len(val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ce08c",
   "metadata": {},
   "source": [
    "## Базовое обучение без RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c1f7",
   "metadata": {},
   "source": [
    "### Функции обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b4bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in enumerate(train_iter):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, \n",
    "                       tgt_input, \n",
    "                       src_mask, \n",
    "                       tgt_mask,\n",
    "                       src_padding_mask, \n",
    "                       tgt_padding_mask, \n",
    "                       src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505923",
   "metadata": {},
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd99cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab)\n",
    "TGT_VOCAB_SIZE = len(vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, \n",
    "                                 NUM_DECODER_LAYERS,\n",
    "                                 EMB_SIZE, SRC_VOCAB_SIZE, \n",
    "                                 TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2d1a2",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ae97015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.585, Val loss: 2.956, Epoch time = 59.204s, All time = 60.737s\n",
      "Epoch: 2, Train loss: 3.446, Val loss: 2.353, Epoch time = 61.746s, All time = 63.469s\n",
      "Epoch: 3, Train loss: 3.006, Val loss: 2.021, Epoch time = 62.291s, All time = 64.029s\n",
      "Epoch: 4, Train loss: 2.695, Val loss: 1.789, Epoch time = 62.128s, All time = 63.668s\n",
      "Epoch: 5, Train loss: 2.447, Val loss: 1.607, Epoch time = 60.461s, All time = 62.186s\n",
      "Epoch: 6, Train loss: 2.238, Val loss: 1.461, Epoch time = 62.401s, All time = 64.144s\n",
      "Epoch: 7, Train loss: 2.055, Val loss: 1.390, Epoch time = 62.170s, All time = 63.888s\n",
      "Epoch: 8, Train loss: 1.897, Val loss: 1.301, Epoch time = 60.197s, All time = 61.928s\n",
      "Epoch: 9, Train loss: 1.755, Val loss: 1.255, Epoch time = 62.284s, All time = 64.039s\n",
      "Epoch: 10, Train loss: 1.632, Val loss: 1.191, Epoch time = 62.313s, All time = 64.033s\n",
      "Epoch: 11, Train loss: 1.525, Val loss: 1.164, Epoch time = 60.097s, All time = 61.862s\n",
      "Epoch: 12, Train loss: 1.425, Val loss: 1.143, Epoch time = 62.284s, All time = 64.007s\n",
      "Epoch: 13, Train loss: 1.334, Val loss: 1.115, Epoch time = 62.361s, All time = 64.103s\n",
      "Epoch: 14, Train loss: 1.255, Val loss: 1.109, Epoch time = 60.130s, All time = 61.865s\n",
      "Epoch: 15, Train loss: 1.185, Val loss: 1.105, Epoch time = 62.416s, All time = 64.126s\n",
      "Epoch: 16, Train loss: 1.117, Val loss: 1.074, Epoch time = 62.341s, All time = 64.052s\n",
      "Epoch: 17, Train loss: 1.060, Val loss: 1.070, Epoch time = 60.455s, All time = 61.984s\n",
      "Epoch: 18, Train loss: 1.006, Val loss: 1.060, Epoch time = 62.196s, All time = 63.935s\n",
      "Epoch: 19, Train loss: 0.958, Val loss: 1.054, Epoch time = 62.208s, All time = 63.947s\n",
      "Epoch: 20, Train loss: 0.914, Val loss: 1.048, Epoch time = 61.973s, All time = 63.512s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer, loss_fn)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    all_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train loss: {train_loss:.3f}, \"\n",
    "          f\"Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s, \"\n",
    "          f\"All time = {(all_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2630c90",
   "metadata": {},
   "source": [
    "### Пример работы (greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6992cdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 97.8 ms, sys: 740 µs, total: 98.5 ms\n",
      "Wall time: 103 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=greedy_decode, num_samples=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348cef0",
   "metadata": {},
   "source": [
    "### Пример работы (multinominal sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d4699f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.2 ms, sys: 22.2 ms, total: 106 ms\n",
      "Wall time: 105 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in our work , we tackle the task of domain adaptation .',\n",
       " 'in this work , we focus on semi - supervised approach .',\n",
       " 'in the inventory presented here , we use a supervised method for domain adaptation .',\n",
       " 'in our work , we investigate the use of word - level features .',\n",
       " 'in our work , we propose a method to improve supervised .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=sampling_decode, num_samples=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf134ba3",
   "metadata": {},
   "source": [
    "## Обучение с RL\n",
    "Продолжаем обучение уже используя модель из предыдущего пункта.\n",
    "\n",
    "В работе предлагается следующий подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16e42fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/LaBSE were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "bert = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(DEVICE)\n",
    "_ = bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c63b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assesor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_best(self, reference, candidates):\n",
    "        with torch.no_grad():\n",
    "            tokes = tokenizer(\n",
    "                [reference], return_tensors='pt', \n",
    "                padding=True, max_length=512, truncation=True).to(DEVICE)\n",
    "            ref_emb = bert(**tokes)[1].cpu().numpy()\n",
    "            tokes = tokenizer(\n",
    "                candidates, return_tensors='pt', \n",
    "                padding=True, max_length=512, truncation=True).to(DEVICE)\n",
    "            can_emb = bert(**tokes)[1].cpu().numpy()\n",
    "\n",
    "        distances = scipy.spatial.distance.cdist(can_emb, \n",
    "                                                 ref_emb,\n",
    "                                                 metric='cosine')\n",
    "    \n",
    "        return np.argmin(distances.reshape(-1))\n",
    "    \n",
    "class Reward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Reward, self).__init__()\n",
    "        self.linear = nn.Linear(1536, 1)\n",
    "        \n",
    "    def forward(self, references, candidates):\n",
    "        assert len(references) == len(candidates)\n",
    "        with torch.no_grad():\n",
    "            tokes = tokenizer(\n",
    "                references, return_tensors='pt', \n",
    "                padding=True, max_length=512, truncation=True).to(DEVICE)\n",
    "            ref_emb = bert(**tokes)[1]\n",
    "            \n",
    "            list_of_features = []\n",
    "            candidates = np.array(candidates)\n",
    "            for i in range(candidates.shape[1]):\n",
    "                tokes = tokenizer(\n",
    "                    candidates[:, i].tolist(), return_tensors='pt', \n",
    "                    padding=True, max_length=512, truncation=True).to(DEVICE)\n",
    "                can_emb = bert(**tokes)[1]\n",
    "\n",
    "                list_of_features.append(torch.cat([ref_emb, can_emb], dim=-1).unsqueeze(1))\n",
    "            \n",
    "            features = torch.cat(list_of_features, dim=1)\n",
    "        return self.linear(features).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42178a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "assesor = Assesor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd00e0",
   "metadata": {},
   "source": [
    "### Генерим выборку для разметки \"асессорами\" + сразу ее разметим \"асессорами\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "114d277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:34<00:00, 10.55it/s]\n"
     ]
    }
   ],
   "source": [
    "assessor_dataset = []\n",
    "for ref in tqdm(train_data[:1000]):\n",
    "    reference = ' '.join([vocab.itos[item] for item in ref[1]])\n",
    "    top_cand = ' '.join([vocab.itos[item] for item in ref[0]])\n",
    "    candidates=paraphrase(\n",
    "        transformer, [reference], \n",
    "        vocab, vocab, tokenizer, decoder=sampling_decode, num_samples=4)[0]\n",
    "    candidates[np.random.randint(0, 4)] = top_cand\n",
    "    assessor_dataset.append((reference, candidates, assesor.get_best(reference, candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66716652",
   "metadata": {},
   "source": [
    "### Обучаем Reward модель на основе асессорской разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b31e36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_reward(data_batch):\n",
    "    ref_batch, cand_batch, tgt_batch = [], [], []\n",
    "    for (ref, candidates, b) in data_batch:\n",
    "        ref_batch.append(ref)\n",
    "        cand_batch.append(candidates)\n",
    "        tgt_batch.append(b)\n",
    "    return ref_batch, cand_batch, tgt_batch\n",
    "\n",
    "train_reward_iter = DataLoader(assessor_dataset, batch_size=64,\n",
    "                               shuffle=True, collate_fn=generate_batch_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f973a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_reward(model, train_iter, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (ref, cands, tgt) in enumerate(train_iter):\n",
    "        logits = model(ref, cands)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = -1*torch.gather(\n",
    "            input=torch.nn.functional.log_softmax(logits, dim=-1), \n",
    "            dim=1,\n",
    "            index=torch.tensor(tgt).long().to(DEVICE).view(-1,1)).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79e0c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = Reward()\n",
    "reward.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    reward.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01c2ba55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.037, Epoch time = 6.884s\n",
      "Epoch: 2, Train loss: 0.463, Epoch time = 7.077s\n",
      "Epoch: 3, Train loss: 0.272, Epoch time = 7.107s\n",
      "Epoch: 4, Train loss: 0.212, Epoch time = 7.261s\n",
      "Epoch: 5, Train loss: 0.192, Epoch time = 7.146s\n",
      "Epoch: 6, Train loss: 0.184, Epoch time = 7.312s\n",
      "Epoch: 7, Train loss: 0.181, Epoch time = 7.330s\n",
      "Epoch: 8, Train loss: 0.178, Epoch time = 7.296s\n",
      "Epoch: 9, Train loss: 0.175, Epoch time = 7.210s\n",
      "Epoch: 10, Train loss: 0.174, Epoch time = 7.295s\n",
      "Epoch: 11, Train loss: 0.172, Epoch time = 7.194s\n",
      "Epoch: 12, Train loss: 0.169, Epoch time = 7.443s\n",
      "Epoch: 13, Train loss: 0.170, Epoch time = 7.433s\n",
      "Epoch: 14, Train loss: 0.169, Epoch time = 7.270s\n",
      "Epoch: 15, Train loss: 0.170, Epoch time = 6.304s\n",
      "Epoch: 16, Train loss: 0.170, Epoch time = 6.164s\n",
      "Epoch: 17, Train loss: 0.172, Epoch time = 6.257s\n",
      "Epoch: 18, Train loss: 0.170, Epoch time = 6.215s\n",
      "Epoch: 19, Train loss: 0.168, Epoch time = 6.373s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch_reward(reward, train_reward_iter, optimizer, None)\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train loss: {train_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ca268",
   "metadata": {},
   "source": [
    "### На основе RL дообучаем модель генерации парафраза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca72c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_rl(model, train_iter, optimizer, loss_fn, alpha=0.75):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in enumerate(train_iter):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, \n",
    "                       tgt_input, \n",
    "                       src_mask, \n",
    "                       tgt_mask,\n",
    "                       src_padding_mask, \n",
    "                       tgt_padding_mask, \n",
    "                       src_padding_mask)\n",
    "        \n",
    "######################RL-start##################################\n",
    "        logits_batch_first = logits.transpose(0,1)\n",
    "        # получаем предсказания для RL\n",
    "        toks = torch.multinomial(\n",
    "            torch.nn.functional.softmax(\n",
    "                logits_batch_first.reshape(-1, logits_batch_first.shape[-1]), \n",
    "                dim=-1), \n",
    "            1).reshape(logits_batch_first.shape[:2])\n",
    "\n",
    "\n",
    "        references = []\n",
    "        candidates = []\n",
    "        for real_toks, pred_toks in zip(tgt[1:,:].transpose(0,1), toks):\n",
    "            reference = []\n",
    "            for tok in real_toks:\n",
    "                if tok.item() == vocab['<eos>']:\n",
    "                    break\n",
    "                if tok.item() not in {vocab['<eos>'], vocab['<bos>'], vocab['<pad>']}:\n",
    "                    reference.append(vocab.itos[tok])\n",
    "            candidate = []\n",
    "            for tok in pred_toks:\n",
    "                if tok.item() == vocab['<eos>']:\n",
    "                    break\n",
    "                if tok.item() not in {vocab['<eos>'], vocab['<bos>'], vocab['<pad>']}:\n",
    "                    candidate.append(vocab.itos[tok])\n",
    "            references.append(' '.join(reference))\n",
    "            candidates.append([' '.join(candidate)])\n",
    "\n",
    "        reward_tr = reward(references, candidates).detach().float().to(DEVICE)\n",
    "\n",
    "        action_proba = torch.gather(torch.nn.functional.log_softmax(logits_batch_first, dim=-1), \n",
    "                                    2, \n",
    "                                    toks.view(*logits_batch_first.shape[:2], 1)).squeeze(-1)\n",
    "######################RL-end###################################\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = alpha*loss_fn(logits.reshape(-1, logits.shape[-1]), tgt[1:,:].reshape(-1)) \\\n",
    "               + (1-alpha)*(-1*reward_tr.view(-1,1)*action_proba).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48ee76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16b0f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.917, Val loss: 1.070, Epoch time = 182.465s, All time = 184.197s\n",
      "Epoch: 2, Train loss: 2.087, Val loss: 1.092, Epoch time = 178.919s, All time = 180.603s\n",
      "Epoch: 3, Train loss: 1.794, Val loss: 1.077, Epoch time = 178.733s, All time = 180.430s\n",
      "Epoch: 4, Train loss: 1.665, Val loss: 1.059, Epoch time = 178.601s, All time = 180.301s\n",
      "Epoch: 5, Train loss: 1.351, Val loss: 1.064, Epoch time = 180.819s, All time = 182.523s\n",
      "Epoch: 6, Train loss: 1.229, Val loss: 1.054, Epoch time = 178.216s, All time = 179.920s\n",
      "Epoch: 7, Train loss: 1.349, Val loss: 1.061, Epoch time = 178.621s, All time = 180.313s\n",
      "Epoch: 8, Train loss: 1.322, Val loss: 1.054, Epoch time = 178.337s, All time = 180.036s\n",
      "Epoch: 9, Train loss: 1.516, Val loss: 1.066, Epoch time = 181.325s, All time = 183.020s\n",
      "Epoch: 10, Train loss: 1.510, Val loss: 1.047, Epoch time = 178.522s, All time = 180.226s\n",
      "Epoch: 11, Train loss: 1.376, Val loss: 1.071, Epoch time = 177.981s, All time = 179.706s\n",
      "Epoch: 12, Train loss: 1.065, Val loss: 1.070, Epoch time = 178.200s, All time = 179.906s\n",
      "Epoch: 13, Train loss: 1.070, Val loss: 1.069, Epoch time = 181.282s, All time = 182.988s\n",
      "Epoch: 14, Train loss: 1.066, Val loss: 1.059, Epoch time = 178.458s, All time = 180.164s\n",
      "Epoch: 15, Train loss: 1.021, Val loss: 1.081, Epoch time = 178.640s, All time = 180.342s\n",
      "Epoch: 16, Train loss: 0.992, Val loss: 1.070, Epoch time = 178.204s, All time = 179.897s\n",
      "Epoch: 17, Train loss: 0.953, Val loss: 1.075, Epoch time = 180.872s, All time = 182.379s\n",
      "Epoch: 18, Train loss: 1.114, Val loss: 1.089, Epoch time = 178.599s, All time = 180.312s\n",
      "Epoch: 19, Train loss: 0.996, Val loss: 1.065, Epoch time = 177.769s, All time = 179.470s\n",
      "Epoch: 20, Train loss: 1.146, Val loss: 1.084, Epoch time = 178.714s, All time = 180.424s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch_with_rl(transformer, train_iter, optimizer, loss_fn)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    all_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train loss: {train_loss:.3f}, \"\n",
    "          f\"Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s, \"\n",
    "          f\"All time = {(all_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35377673",
   "metadata": {},
   "source": [
    "### Пример работы (greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e10116cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the learning can be viewed as a special case of the .',\n",
       " 'the learning can be viewed as a special case of the .',\n",
       " 'the learning can be viewed as a special case of the .',\n",
       " 'the learning can be viewed as a special case of the .',\n",
       " 'the learning can be viewed as a special case of the .']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=greedy_decode, num_samples=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c64e2",
   "metadata": {},
   "source": [
    "### Пример работы (multinominal sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fdbdc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matching embeddings at 6 first level can be while better suited to latter classes',\n",
       " 'had 5 distributional .',\n",
       " 'capacity and anaphoric learning can be adopt to structure generation .',\n",
       " 'probabilistic labels can be had by aligning translations .',\n",
       " 'be several generation methods because the morphemes can issues .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=sampling_decode, num_samples=5)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
