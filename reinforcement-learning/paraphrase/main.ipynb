{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90bb1b9",
   "metadata": {},
   "source": [
    "# Paraphrase Generation with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ebf80",
   "metadata": {},
   "source": [
    "Обзор работы по использованию обучения с подкреплением для задачи машинного перевода. Статья по [ссылке](https://www.aclweb.org/anthology/D18-1421.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ccd98",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ba4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a3f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убирем рандом\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5788d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1a913",
   "metadata": {},
   "source": [
    "## Базовый пример обучения генерации парафраза\n",
    "[На основе кода по генерации перевода.](https://pytorch.org/tutorials/beginner/translation_transformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d3939",
   "metadata": {},
   "source": [
    "### Скачиваем данные для парафраза на инглийском.\n",
    "[Статья 2021 года. ParaSCI: A Large Scientific Paraphrase Dataset for Longer Paraphrase Generation.](https://github.com/dqxiu/ParaSCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370e898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/dqxiu/ParaSCI/master/Data/ParaSCI-ACL/'\n",
    "train_urls = ('train/train.src', 'train/train.tgt')\n",
    "val_urls = ('val/val.src', 'val/val.tgt')\n",
    "test_urls = ('test/test.src', 'test/test.tgt')\n",
    "\n",
    "train_filepaths = [download_from_url(url_base + url) for url in train_urls]\n",
    "val_filepaths = [download_from_url(url_base + url) for url in val_urls]\n",
    "test_filepaths = [download_from_url(url_base + url) for url in test_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c018b5b",
   "metadata": {},
   "source": [
    "### Строим словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d00d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def build_vocab(filepaths, tokenizer):\n",
    "    counter = Counter()\n",
    "    for filepath in filepaths:\n",
    "        with io.open(filepath, encoding=\"utf8\") as f:\n",
    "            for string_ in f:\n",
    "                counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "vocab = build_vocab(train_filepaths, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c17ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14821"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d30be2",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2dd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_tgt_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for (raw_src, raw_tgt) in zip(raw_src_iter, raw_tgt_iter):\n",
    "        src_tensor_ = torch.tensor(\n",
    "            [vocab[token] for token in tokenizer(raw_src.rstrip(\"\\n\"))],\n",
    "            dtype=torch.long)\n",
    "        tgt_tensor_ = torch.tensor(\n",
    "            [vocab[token] for token in tokenizer(raw_tgt.rstrip(\"\\n\"))],\n",
    "            dtype=torch.long)\n",
    "        data.append((src_tensor_, tgt_tensor_))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09e4a3",
   "metadata": {},
   "source": [
    "### Финальные датасеты для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48b8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "PAD_IDX = vocab['<pad>']\n",
    "BOS_IDX = vocab['<bos>']\n",
    "EOS_IDX = vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f148047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for (src_item, tgt_item) in data_batch:\n",
    "        src_batch.append(torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        tgt_batch.append(torch.cat([torch.tensor([BOS_IDX]), tgt_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6799c3",
   "metadata": {},
   "source": [
    "### Класс модели SEQ2SEQ transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff66cc",
   "metadata": {},
   "source": [
    "#### Определение самой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03183b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
    "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
    "                                        tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding +\n",
    "                            self.pos_embedding[:token_embedding.size(0),:])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "# Делаем, так чтобы в обучении не было заглядывания на дальнешие слова\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d1258",
   "metadata": {},
   "source": [
    "#### Определения декодирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0a8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, num_samples=1):\n",
    "    src = src.to(DEVICE)\n",
    "    src = torch.cat([src]*num_samples, dim=1)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, num_samples).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(DEVICE).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.detach()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        next_word.view(1, -1)], dim=0)\n",
    "    return ys.transpose(0,1)\n",
    "\n",
    "def sampling_decode(model, src, src_mask, max_len, start_symbol, num_samples=1):\n",
    "    src = src.to(DEVICE)\n",
    "    src = torch.cat([src]*num_samples, dim=1)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, num_samples).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(DEVICE).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = torch.multinomial(torch.nn.functional.softmax(prob, dim=-1), 1)\n",
    "        next_word = next_word.detach()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        next_word.view(1, -1)], dim=0)\n",
    "    return ys.transpose(0,1)\n",
    "\n",
    "def paraphrase(model, \n",
    "              srcs, \n",
    "              src_vocab, \n",
    "              tgt_vocab, \n",
    "              src_tokenizer, \n",
    "              decoder=greedy_decode, \n",
    "              ret_tokens=False, \n",
    "              ret_idx=False, \n",
    "              max_len_add=10,\n",
    "              input_idx=False,\n",
    "              **argv):\n",
    "    model.eval()\n",
    "    global_answers = []\n",
    "    for src in srcs:\n",
    "        if not input_idx:\n",
    "            tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
    "            src = torch.LongTensor(tokens)\n",
    "        num_tokens = len(src)\n",
    "        src = src.reshape(num_tokens, 1)\n",
    "        \n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "        tgt_tokens = decoder(model, src, src_mask, max_len=num_tokens + max_len_add, start_symbol=BOS_IDX, **argv)\n",
    "\n",
    "        answers = []\n",
    "        for tgt_token in tgt_tokens:\n",
    "            if not ret_idx:\n",
    "                reference = []\n",
    "                for tok in tgt_token:\n",
    "                    if tok.item() == tgt_vocab['<eos>']:\n",
    "                        break\n",
    "                    if tok.item() not in {tgt_vocab['<eos>'], tgt_vocab['<bos>'], tgt_vocab['<pad>']}:\n",
    "                        reference.append(tgt_vocab.itos[tok])\n",
    "                answers.append(\" \".join(reference).strip())\n",
    "                if ret_tokens:\n",
    "                    answers[-1] = answers[-1].split(\" \")\n",
    "            else:\n",
    "                reference = []\n",
    "                for tok in tgt_token:\n",
    "                    if tok.item() == tgt_vocab['<eos>']:\n",
    "                        break\n",
    "                    if tok.item() not in {tgt_vocab['<eos>'], tgt_vocab['<bos>'], tgt_vocab['<pad>']}:\n",
    "                        reference.append(tok.item())\n",
    "                        \n",
    "                answers.append(reference)\n",
    "        global_answers.append(answers)\n",
    "    return global_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d37fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                              src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses / len(val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ce08c",
   "metadata": {},
   "source": [
    "## Базовое обучение без RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c1f7",
   "metadata": {},
   "source": [
    "### Функции обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b4bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in enumerate(train_iter):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, \n",
    "                       tgt_input, \n",
    "                       src_mask, \n",
    "                       tgt_mask,\n",
    "                       src_padding_mask, \n",
    "                       tgt_padding_mask, \n",
    "                       src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505923",
   "metadata": {},
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd99cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab)\n",
    "TGT_VOCAB_SIZE = len(vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, \n",
    "                                 NUM_DECODER_LAYERS,\n",
    "                                 EMB_SIZE, SRC_VOCAB_SIZE, \n",
    "                                 TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2d1a2",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ae97015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.585, Val loss: 2.956, Epoch time = 46.839s, All time = 48.204s\n",
      "Epoch: 2, Train loss: 3.446, Val loss: 2.353, Epoch time = 47.250s, All time = 48.622s\n",
      "Epoch: 3, Train loss: 3.006, Val loss: 2.021, Epoch time = 47.126s, All time = 48.496s\n",
      "Epoch: 4, Train loss: 2.695, Val loss: 1.789, Epoch time = 47.338s, All time = 48.686s\n",
      "Epoch: 5, Train loss: 2.447, Val loss: 1.607, Epoch time = 47.526s, All time = 48.876s\n",
      "Epoch: 6, Train loss: 2.238, Val loss: 1.461, Epoch time = 47.660s, All time = 49.024s\n",
      "Epoch: 7, Train loss: 2.055, Val loss: 1.390, Epoch time = 47.774s, All time = 49.241s\n",
      "Epoch: 8, Train loss: 1.897, Val loss: 1.301, Epoch time = 47.755s, All time = 49.102s\n",
      "Epoch: 9, Train loss: 1.755, Val loss: 1.255, Epoch time = 47.286s, All time = 48.647s\n",
      "Epoch: 10, Train loss: 1.632, Val loss: 1.191, Epoch time = 47.471s, All time = 48.833s\n",
      "Epoch: 11, Train loss: 1.525, Val loss: 1.164, Epoch time = 47.395s, All time = 48.770s\n",
      "Epoch: 12, Train loss: 1.425, Val loss: 1.143, Epoch time = 47.532s, All time = 48.928s\n",
      "Epoch: 13, Train loss: 1.334, Val loss: 1.115, Epoch time = 47.441s, All time = 48.804s\n",
      "Epoch: 14, Train loss: 1.255, Val loss: 1.109, Epoch time = 47.308s, All time = 48.693s\n",
      "Epoch: 15, Train loss: 1.185, Val loss: 1.105, Epoch time = 47.057s, All time = 48.411s\n",
      "Epoch: 16, Train loss: 1.117, Val loss: 1.074, Epoch time = 47.538s, All time = 48.895s\n",
      "Epoch: 17, Train loss: 1.060, Val loss: 1.070, Epoch time = 47.317s, All time = 48.661s\n",
      "Epoch: 18, Train loss: 1.006, Val loss: 1.060, Epoch time = 47.041s, All time = 48.350s\n",
      "Epoch: 19, Train loss: 0.958, Val loss: 1.054, Epoch time = 46.811s, All time = 48.124s\n",
      "Epoch: 20, Train loss: 0.914, Val loss: 1.048, Epoch time = 46.631s, All time = 47.973s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer, loss_fn)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    all_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train loss: {train_loss:.3f}, \"\n",
    "          f\"Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s, \"\n",
    "          f\"All time = {(all_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2630c90",
   "metadata": {},
   "source": [
    "### Пример работы (greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6992cdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.6 ms, sys: 592 µs, total: 84.2 ms\n",
      "Wall time: 90.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .',\n",
       " 'in our work , we investigate the use of domain adaptation .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=greedy_decode, num_samples=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348cef0",
   "metadata": {},
   "source": [
    "### Пример работы (multinominal sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d4699f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.1 ms, sys: 0 ns, total: 88.1 ms\n",
      "Wall time: 94.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in our work , we tackle the task of domain adaptation .',\n",
       " 'in this work , we focus on semi - supervised approach .',\n",
       " 'in the inventory presented here , we use a supervised method for domain adaptation .',\n",
       " 'in our work , we investigate the use of word - level features .',\n",
       " 'in our work , we propose a method to improve supervised .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=sampling_decode, num_samples=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf134ba3",
   "metadata": {},
   "source": [
    "## Обучение с RL\n",
    "Продолжаем обучение уже используя модель из предыдущего пункта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1960549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "        self.bert = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(DEVICE)\n",
    "        self.bert.eval()\n",
    "        \n",
    "    def score(self, references, candidates):\n",
    "        assert len(references) == len(candidates)\n",
    "        with torch.no_grad():\n",
    "            tokes = reward.tokenizer(\n",
    "                references, return_tensors='pt', \n",
    "                padding=True, max_length=512, truncation=True).to(DEVICE)\n",
    "            ref_emb = reward.bert(**tokes)[1].cpu().numpy()\n",
    "            tokes = reward.tokenizer(\n",
    "                candidates, return_tensors='pt', \n",
    "                padding=True, max_length=512, truncation=True).to(DEVICE)\n",
    "            can_emb = reward.bert(**tokes)[1].cpu().numpy()\n",
    "\n",
    "        distances = 1-scipy.spatial.distance.cdist(can_emb, \n",
    "                                                   ref_emb,\n",
    "                                                   metric='cosine').diagonal()\n",
    "    \n",
    "        return distances.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f894fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/LaBSE were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "reward = Reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1beae531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_rl(model, train_iter, optimizer, loss_fn, alpha=0.75):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in enumerate(train_iter):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, \n",
    "                       tgt_input, \n",
    "                       src_mask, \n",
    "                       tgt_mask,\n",
    "                       src_padding_mask, \n",
    "                       tgt_padding_mask, \n",
    "                       src_padding_mask)\n",
    "        \n",
    "######################RL-start##################################\n",
    "        logits_batch_first = logits.transpose(0,1)\n",
    "        # получаем предсказания для RL\n",
    "        toks = torch.multinomial(\n",
    "            torch.nn.functional.softmax(\n",
    "                logits_batch_first.reshape(-1, logits_batch_first.shape[-1]), \n",
    "                dim=-1), \n",
    "            1).reshape(logits_batch_first.shape[:2])\n",
    "\n",
    "\n",
    "        references = []\n",
    "        candidates = []\n",
    "        for real_toks, pred_toks in zip(tgt[1:,:].transpose(0,1), toks):\n",
    "            reference = []\n",
    "            for tok in real_toks:\n",
    "                if tok.item() == vocab['<eos>']:\n",
    "                    break\n",
    "                if tok.item() not in {vocab['<eos>'], vocab['<bos>'], vocab['<pad>']}:\n",
    "                    reference.append(vocab.itos[tok])\n",
    "            candidate = []\n",
    "            for tok in pred_toks:\n",
    "                if tok.item() == vocab['<eos>']:\n",
    "                    break\n",
    "                if tok.item() not in {vocab['<eos>'], vocab['<bos>'], vocab['<pad>']}:\n",
    "                    candidate.append(vocab.itos[tok])\n",
    "            references.append(' '.join(reference))\n",
    "            candidates.append(' '.join(candidate))\n",
    "\n",
    "        reward_tr = torch.tensor(reward.score(references, candidates)).float().to(DEVICE)\n",
    "\n",
    "        action_proba = torch.gather(torch.nn.functional.log_softmax(logits_batch_first, dim=-1), \n",
    "                                    2, \n",
    "                                    toks.view(*logits_batch_first.shape[:2], 1)).squeeze(-1)\n",
    "######################RL-end###################################\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = alpha*loss_fn(logits.reshape(-1, logits.shape[-1]), tgt[1:,:].reshape(-1)) \\\n",
    "               + (1-alpha)*(-1*reward_tr.view(-1,1)*action_proba).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d3cca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.190, Val loss: 1.045, Epoch time = 160.892s, All time = 162.248s\n",
      "Epoch: 2, Train loss: 0.873, Val loss: 1.046, Epoch time = 161.845s, All time = 163.209s\n",
      "Epoch: 3, Train loss: 0.762, Val loss: 1.040, Epoch time = 161.958s, All time = 163.291s\n",
      "Epoch: 4, Train loss: 0.829, Val loss: 1.055, Epoch time = 160.823s, All time = 162.148s\n",
      "Epoch: 5, Train loss: 0.854, Val loss: 1.037, Epoch time = 160.619s, All time = 161.958s\n",
      "Epoch: 6, Train loss: 0.753, Val loss: 1.042, Epoch time = 161.538s, All time = 162.873s\n",
      "Epoch: 7, Train loss: 0.683, Val loss: 1.052, Epoch time = 160.725s, All time = 162.044s\n",
      "Epoch: 8, Train loss: 0.828, Val loss: 1.039, Epoch time = 161.223s, All time = 162.538s\n",
      "Epoch: 9, Train loss: 0.933, Val loss: 1.044, Epoch time = 161.464s, All time = 162.835s\n",
      "Epoch: 10, Train loss: 0.722, Val loss: 1.048, Epoch time = 160.146s, All time = 161.465s\n",
      "Epoch: 11, Train loss: 0.779, Val loss: 1.044, Epoch time = 160.257s, All time = 161.583s\n",
      "Epoch: 12, Train loss: 0.797, Val loss: 1.076, Epoch time = 161.026s, All time = 162.343s\n",
      "Epoch: 13, Train loss: 0.638, Val loss: 1.087, Epoch time = 160.404s, All time = 161.714s\n",
      "Epoch: 14, Train loss: 0.613, Val loss: 1.064, Epoch time = 161.224s, All time = 162.520s\n",
      "Epoch: 15, Train loss: 0.704, Val loss: 1.094, Epoch time = 160.319s, All time = 161.650s\n",
      "Epoch: 16, Train loss: 0.783, Val loss: 1.069, Epoch time = 160.904s, All time = 162.205s\n",
      "Epoch: 17, Train loss: 0.652, Val loss: 1.066, Epoch time = 161.072s, All time = 162.400s\n",
      "Epoch: 18, Train loss: 0.567, Val loss: 1.074, Epoch time = 161.049s, All time = 162.373s\n",
      "Epoch: 19, Train loss: 0.509, Val loss: 1.083, Epoch time = 161.658s, All time = 162.974s\n",
      "Epoch: 20, Train loss: 0.588, Val loss: 1.074, Epoch time = 160.443s, All time = 161.794s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch_with_rl(transformer, train_iter, optimizer, loss_fn)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    all_time = time.time()\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train loss: {train_loss:.3f}, \"\n",
    "          f\"Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s, \"\n",
    "          f\"All time = {(all_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15863280",
   "metadata": {},
   "source": [
    "### Пример работы (greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df46c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.3 ms, sys: 326 µs, total: 83.6 ms\n",
      "Wall time: 88.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in this paper , we propose a method to reduce the domain adaptation of the considered one .',\n",
       " 'in this paper , we propose a method to reduce the domain adaptation of the considered one .',\n",
       " 'in this paper , we propose a method to reduce the domain adaptation of the considered one .',\n",
       " 'in this paper , we propose a method to reduce the domain adaptation of the considered one .',\n",
       " 'in this paper , we propose a method to reduce the domain adaptation of the considered one .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=greedy_decode, num_samples=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3a749",
   "metadata": {},
   "source": [
    "### Пример работы (multinominal sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32bb048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.6 ms, sys: 4.08 ms, total: 86.6 ms\n",
      "Wall time: 92 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in this study , we propose a method to efficiently optimize the domain adaptation problem .',\n",
       " 'in our paper , we investigate domain adaptation .',\n",
       " 'in this study , we focus on supervised adaptation .',\n",
       " 'in our work , we investigate domain adaptation for supervised adaptation .',\n",
       " 'in our work , we leverage the domain adaptation problem .']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "paraphrase(transformer, [\"in our work , we focus on supervised domain adaptation .\"], \n",
    "          vocab, \n",
    "          vocab, \n",
    "          tokenizer, \n",
    "          decoder=sampling_decode, num_samples=5)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
